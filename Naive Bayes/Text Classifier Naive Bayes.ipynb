{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566\n"
     ]
    }
   ],
   "source": [
    "redundant_titles = ([\"xref:\",\"path:\",\"from:\",\"newsgroups:\",\"subject:\",\"summary:\",\"keywords:\",\"message-id:\",\"date:\",\"expires:\",\"followup-to:\",\n",
    "                  \"distribution:\",\"organization:\",\"organization:\",\"approved:\",\"supersedes:\",\"lines:\",\"archive-name:\",\"last-modified:\",\"version:\"\n",
    "                 \"references:\",\"x-newsreader:\",\"write to:\",\"telephone:\",\"fax:\",\"source\",\"references:\",\"writes:\",\">the\",\"sender:\",\n",
    "                    \"reply-to:\",\"things\",\"number\",\"work\",'(usenet','time','article','good','however,''fri,','tue,'])\n",
    "stop_words.extend(redundant_titles)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = {}\n",
    "def add_to_dict(clean_text): #Adding words and increasing the frequency if already present in dictionary simultaneously\n",
    "    for w in clean_text:\n",
    "        if w not in word_dictionary:\n",
    "            word_dictionary[w] = 1\n",
    "        elif w in word_dictionary:\n",
    "            word_dictionary[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.split() #Words from a particular text file\n",
    "    clean_text = []\n",
    "    #Function adds all non-stop words into a list clean_text\n",
    "    for w in text:\n",
    "        w = w.lower()\n",
    "        if w not in stop_words:\n",
    "            if len(w) > 3 and len(w) < 16:\n",
    "                clean_text.append(w) \n",
    "    #Call to add to dictionary\n",
    "    add_to_dict(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\alt.atheism\n",
      "2 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.graphics\n",
      "3 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.os.ms-windows.misc\n",
      "4 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.sys.ibm.pc.hardware\n",
      "5 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.sys.mac.hardware\n",
      "6 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.windows.x\n",
      "7 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\misc.forsale\n",
      "8 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.autos\n",
      "9 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.motorcycles\n",
      "10 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.sport.baseball\n",
      "11 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.sport.hockey\n",
      "12 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.crypt\n",
      "13 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.electronics\n",
      "14 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.med\n",
      "15 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.space\n",
      "16 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\soc.religion.christian\n",
      "17 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.guns\n",
      "18 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.mideast\n",
      "19 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.misc\n",
      "20 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r\"C:\\Users\\Karmanya\\Desktop\\20_newsgroups\"\n",
    "dirs = [f for f in os.listdir(path)]\n",
    "\n",
    "count_txt_files = 0\n",
    "folder_no = 0\n",
    "y_temp = []\n",
    "\n",
    "for folder in dirs:\n",
    "    path_folder = os.path.join(path,folder)\n",
    "    \n",
    "    folder_no += 1\n",
    "    print(folder_no,end = \" \")\n",
    "    print(path_folder)\n",
    "    \n",
    "    \n",
    "    files = [f for f in os.listdir(path_folder)]\n",
    "    for single_file in files:\n",
    "        path_txt_file = os.path.join(path_folder,single_file)\n",
    "        document = open(path_txt_file,'r',errors='ignore')\n",
    "        \n",
    "        text = document.read()\n",
    "        clean(text)\n",
    "        \n",
    "        y_temp.append(folder_no)  #Updating y_train values with 1-20 as the possible classes\n",
    "        count_txt_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 1)\n",
      "   Class\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "5      1\n",
      "6      1\n",
      "7      1\n",
      "8      1\n",
      "9      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_data = pd.DataFrame(y_temp,columns = [\"Class\"] )\n",
    "print(y_data.shape)\n",
    "print(y_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997,)\n"
     ]
    }
   ],
   "source": [
    "y_data2 = pd.Series(y_temp)\n",
    "print(y_data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299459\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with tuples containing word-value pair sorted in descending order\n",
    "word_dictionary_sorted = sorted(word_dictionary.items(), key = lambda x : x[1] ,reverse = True) \n",
    "#Here lambda is an inline function which returns the value from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1993', 14414)\n",
      "('people', 8415)\n",
      "('university', 8203)\n",
      "('computer', 3157)\n",
      "('news', 2836)\n",
      "('state', 2787)\n",
      "('government', 2534)\n",
      "('problem', 2528)\n",
      "('read', 2516)\n",
      "('fri,', 2307)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(word_dictionary_sorted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "words= []\n",
    "index_list = []\n",
    "for i in range(len(word_dictionary_sorted)):\n",
    "    values.append(word_dictionary_sorted[i][1])\n",
    "    words.append(word_dictionary_sorted[i][0])\n",
    "    index_list.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(index_list,values)\n",
    "plt.axis([0,15000,1,1000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictionary = {}\n",
    "max_words = 1000\n",
    "#Considering the top 1000 most occuring words from word_dictionary_sorted which contains tuple pairs of form (word,count)\n",
    "final_dictionary = word_dictionary_sorted[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of top 1000 words only\n",
    "value_list = []\n",
    "for i in range(len(final_dictionary)):\n",
    "    value_list.append(final_dictionary[i][1])\n",
    "\n",
    "#Top 1000 words only-these will be the features for x_train    \n",
    "word_list= []\n",
    "for i in range(len(final_dictionary)):\n",
    "    word_list.append(final_dictionary[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Idea is to construct X in such a way that the highest occuring word is the 1st column and sequentially there after\n",
    "import numpy as np\n",
    "rows = count_txt_files\n",
    "cols = len(final_dictionary)\n",
    "x_np = np.zeros(shape = (rows,cols))\n",
    "print(x_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_training(text,document_no):\n",
    "    #here document_no is the row number \n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if word in word_list:\n",
    "            index = word_list.index(word) #Calculating the index of the word stored in ordered list by descending order of count\n",
    "            x_np[document_no][index] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\alt.atheism\n",
      "2 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.graphics\n",
      "3 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.os.ms-windows.misc\n",
      "4 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.sys.ibm.pc.hardware\n",
      "5 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.sys.mac.hardware\n",
      "6 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\comp.windows.x\n",
      "7 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\misc.forsale\n",
      "8 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.autos\n",
      "9 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.motorcycles\n",
      "10 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.sport.baseball\n",
      "11 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\rec.sport.hockey\n",
      "12 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.crypt\n",
      "13 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.electronics\n",
      "14 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.med\n",
      "15 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\sci.space\n",
      "16 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\soc.religion.christian\n",
      "17 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.guns\n",
      "18 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.mideast\n",
      "19 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.politics.misc\n",
      "20 C:\\Users\\Karmanya\\Desktop\\20_newsgroups\\talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r\"C:\\Users\\Karmanya\\Desktop\\20_newsgroups\"\n",
    "dirs = [f for f in os.listdir(path)]\n",
    "\n",
    "document_no = 0 #Counting the index of the documents/text files\n",
    "folder_no = 0\n",
    "\n",
    "for folder in dirs:\n",
    "    path_folder = os.path.join(path,folder)\n",
    "    \n",
    "    folder_no += 1\n",
    "    print(folder_no,end = \" \")\n",
    "    print(path_folder)\n",
    "    \n",
    "    files = [f for f in os.listdir(path_folder)]\n",
    "    for single_file in files:\n",
    "        path_txt_file = os.path.join(path_folder,single_file)\n",
    "        document = open(path_txt_file,'r',errors='ignore')\n",
    "        text = document.read()\n",
    "        add_to_training(text,document_no)\n",
    "        document_no += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1993</th>\n",
       "      <th>people</th>\n",
       "      <th>university</th>\n",
       "      <th>computer</th>\n",
       "      <th>news</th>\n",
       "      <th>state</th>\n",
       "      <th>government</th>\n",
       "      <th>problem</th>\n",
       "      <th>read</th>\n",
       "      <th>fri,</th>\n",
       "      <th>...</th>\n",
       "      <th>beat</th>\n",
       "      <th>comment</th>\n",
       "      <th>throw</th>\n",
       "      <th>radar</th>\n",
       "      <th>corporation,</th>\n",
       "      <th>secure</th>\n",
       "      <th>drop</th>\n",
       "      <th>article,</th>\n",
       "      <th>(andrew</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1993  people  university  computer  news  state  government  problem  read  \\\n",
       "0   2.0     3.0         0.0       0.0   0.0    0.0         0.0      0.0   1.0   \n",
       "1   3.0    18.0         0.0       0.0   0.0    6.0         1.0      2.0   3.0   \n",
       "2   1.0     1.0         0.0       0.0   1.0    0.0         0.0      1.0   1.0   \n",
       "3   1.0     0.0         1.0       0.0   0.0    0.0         0.0      0.0   0.0   \n",
       "4   0.0     0.0         0.0       0.0   0.0    0.0         0.0      0.0   0.0   \n",
       "\n",
       "   fri,   ...    beat  comment  throw  radar  corporation,  secure  drop  \\\n",
       "0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   0.0   \n",
       "1   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   0.0   \n",
       "2   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   0.0   \n",
       "3   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   0.0   \n",
       "4   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   0.0   \n",
       "\n",
       "   article,  (andrew  escape  \n",
       "0       0.0      0.0     0.0  \n",
       "1       0.0      0.0     0.0  \n",
       "2       0.0      0.0     0.0  \n",
       "3       0.0      0.0     0.0  \n",
       "4       0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Constructing a pandas dataframe with the columns name as the top 1000 words\n",
    "x_data = pd.DataFrame(data = x_np, columns=word_list )\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,random_state = 0,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1993</th>\n",
       "      <th>people</th>\n",
       "      <th>university</th>\n",
       "      <th>computer</th>\n",
       "      <th>news</th>\n",
       "      <th>state</th>\n",
       "      <th>government</th>\n",
       "      <th>problem</th>\n",
       "      <th>read</th>\n",
       "      <th>fri,</th>\n",
       "      <th>...</th>\n",
       "      <th>beat</th>\n",
       "      <th>comment</th>\n",
       "      <th>throw</th>\n",
       "      <th>radar</th>\n",
       "      <th>corporation,</th>\n",
       "      <th>secure</th>\n",
       "      <th>drop</th>\n",
       "      <th>article,</th>\n",
       "      <th>(andrew</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18608</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1993  people  university  computer  news  state  government  problem  \\\n",
       "18608   1.0     0.0         0.0       0.0   0.0    0.0         0.0      0.0   \n",
       "19452   1.0     0.0         0.0       0.0   0.0    0.0         0.0      0.0   \n",
       "12831   1.0     0.0         0.0       0.0   0.0    0.0         0.0      3.0   \n",
       "6664    1.0     0.0         0.0       0.0   0.0    0.0         0.0      0.0   \n",
       "12603   1.0     0.0         0.0       0.0   0.0    0.0         0.0      0.0   \n",
       "\n",
       "       read  fri,   ...    beat  comment  throw  radar  corporation,  secure  \\\n",
       "18608   0.0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   \n",
       "19452   0.0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   \n",
       "12831   0.0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   \n",
       "6664    0.0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   \n",
       "12603   0.0   0.0   ...     0.0      0.0    0.0    0.0           0.0     0.0   \n",
       "\n",
       "       drop  article,  (andrew  escape  \n",
       "18608   0.0       0.0      0.0     0.0  \n",
       "19452   0.0       0.0      0.0     0.0  \n",
       "12831   1.0       0.0      0.0     0.0  \n",
       "6664    0.0       0.0      0.0     0.0  \n",
       "12603   0.0       0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = np.finfo(float).eps\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODING THE MULTINOMIAL NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10766.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 1993\n",
    "x_train.iloc[:,2].sum()\n",
    "x_train.loc[:,str(name)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_func(x_train,y_train):\n",
    "    result = {} #dictionary to store the count of words occuring in distinct classes\n",
    "    all_classes = y_train[\"Class\"].unique().tolist() #Distinct classes\n",
    "    \n",
    "    result[\"total_size\"] = len(y_train)\n",
    "    for cur_class in all_classes:\n",
    "        \n",
    "        result[cur_class] = {}\n",
    "        \n",
    "        cur_class_rows = (y_train.Class == cur_class) #Selecting all the rows with current class\n",
    "        x_train_cur = x_train[cur_class_rows]\n",
    "        y_train_cur = y_train[cur_class_rows]\n",
    "        result[cur_class][\"total_class_count\"] = y_train_cur.sum()[0]\n",
    "        #print(result[cur_class][\"total_class_count\"])\n",
    "        \n",
    "        num_words = x_train.shape[1]\n",
    "        total = 0\n",
    "        for j in range(num_words): \n",
    "            result[cur_class][j] = {}\n",
    "            count_word = x_train_cur.iloc[:,j].sum()\n",
    "            result[cur_class][j] = count_word\n",
    "            total += count_word\n",
    "            \n",
    "        result[cur_class][\"total_word_count\"] = total #storing the total count of words in a specific class\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test,dictionary):\n",
    "    \n",
    "    #Here x_test are all the documents which need to have text classified\n",
    "    #dictionary is result which stores the count of words\n",
    "    y_pred = []\n",
    "    \n",
    "    rows = x_test.shape[0]\n",
    "    for i in range(rows):\n",
    "        x = x_test.iloc[i,:]\n",
    "        x_class = predict_single(x,dictionary)\n",
    "        y_pred.append(x_class)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x,dictionary):\n",
    "    \n",
    "    #Here we predict the class for a single document using a probability function\n",
    "    best_p = -100.0\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    \n",
    "    #Iterate over all the classes,-> dictionary keys\n",
    "    class_vals = set(dictionary.keys())\n",
    "    #Calculate the probability for each class and select the one with maximum probability\n",
    "    for cur_class in class_vals:\n",
    "        \n",
    "        if cur_class != \"total_size\":  #Excluding total size as a the class for avoiding errors\n",
    "            calc_prob = prob_class(x,cur_class,dictionary)\n",
    "            \n",
    "            if first_run == True:\n",
    "                best_p = calc_prob\n",
    "                best_class = cur_class\n",
    "                first_run = False\n",
    "                \n",
    "            if calc_prob > best_p:\n",
    "                best_p = calc_prob\n",
    "                best_class = cur_class\n",
    "                \n",
    "    \n",
    "    return best_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_class(x,cval,dictionary):\n",
    "    #Calculating probability y = cval initially \n",
    "    prob = np.log(dictionary[cval][\"total_class_count\"] + eps) - np.log(dictionary[\"total_size\"] + eps)\n",
    "    #print(prob,end = \" \")\n",
    "    \n",
    "    #Calculating probability \n",
    "    for i in range(x.shape[0]):\n",
    "        x_word_count = x[i]\n",
    "        if x_word_count > 0:\n",
    "            word_class_count = dictionary[cval][i] + 1 \n",
    "            word_count_cval = dictionary[cval][\"total_word_count\"] + x.shape[0]\n",
    "            prob_word = np.log(word_class_count + eps)-np.log(word_count_cval + eps)\n",
    "            prob += prob_word\n",
    "            \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karmanya\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred_sklearn = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.81      0.71       233\n",
      "           2       0.74      0.78      0.76       253\n",
      "           3       0.60      0.48      0.53       249\n",
      "           4       0.59      0.54      0.57       240\n",
      "           5       0.54      0.46      0.50       236\n",
      "           6       0.78      0.86      0.82       240\n",
      "           7       0.64      0.80      0.71       261\n",
      "           8       0.80      0.89      0.84       269\n",
      "           9       0.77      0.93      0.84       284\n",
      "          10       0.67      0.59      0.63       248\n",
      "          11       0.71      0.71      0.71       231\n",
      "          12       0.85      0.85      0.85       233\n",
      "          13       0.82      0.80      0.81       244\n",
      "          14       0.75      0.84      0.80       256\n",
      "          15       0.80      0.80      0.80       246\n",
      "          16       0.70      0.63      0.66       252\n",
      "          17       0.53      0.61      0.57       249\n",
      "          18       0.68      0.57      0.62       281\n",
      "          19       0.55      0.44      0.49       259\n",
      "          20       0.38      0.28      0.33       236\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      5000\n",
      "   macro avg       0.68      0.68      0.68      5000\n",
      "weighted avg       0.68      0.69      0.68      5000\n",
      "\n",
      "[[188   1   0   0   0   0   0   0   1   0   1   0   0   2   1   5   6   1\n",
      "    1  26]\n",
      " [  2 197  14   5   4   8   3   2   1   3   1   0   0   2   6   0   1   2\n",
      "    0   2]\n",
      " [  2  18 119  20  13  23  13   4   6   3   3   4   4   4   7   1   2   0\n",
      "    1   2]\n",
      " [  0   9  19 130  41   3  13   7   5   0   2   0   7   2   0   2   0   0\n",
      "    0   0]\n",
      " [  0   1  20  39 109   6  19   8   3   2   1   2  13   5   1   1   5   1\n",
      "    0   0]\n",
      " [  0  11   5   2   3 206   4   1   2   0   0   2   2   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   1   3  13   9   0 210   7   1   2   1   0   3   1   6   0   0   2\n",
      "    1   1]\n",
      " [  0   1   3   0   2   1   4 240   3   1   0   0   3   0   0   2   6   2\n",
      "    1   0]\n",
      " [  1   0   0   0   1   1   2   3 263   0   0   0   0   3   1   0   3   1\n",
      "    4   1]\n",
      " [  5   1   2   0   3   3  17   3   8 147  42   1   0   4   1   1   1   4\n",
      "    4   1]\n",
      " [  2   1   1   0   1   1   5   1   4  33 165   2   0   2   2   1   4   3\n",
      "    2   1]\n",
      " [  2   1   0   1   2   5   1   1   4   0   0 197   2   2   0   1   5   1\n",
      "    5   3]\n",
      " [  0   5   4   5   5   1   9   6   5   1   1   0 196   4   1   0   0   0\n",
      "    0   1]\n",
      " [  2   4   1   0   3   0   5   2   4   3   4   0   0 216   2   1   2   3\n",
      "    2   2]\n",
      " [  3   6   0   0   1   4   3   3   4   2   1   1   4   5 197   1   3   2\n",
      "    1   5]\n",
      " [ 18   1   1   2   1   0   2   1   3   4   4   3   1   8   4 159   7  11\n",
      "    8  14]\n",
      " [  1   2   2   1   0   1   6   3   8   3   2   8   2   8   1   5 152   9\n",
      "   21  14]\n",
      " [ 12   3   1   0   1   1   6   1   5   6   2   7   1   6   5   8  16 160\n",
      "   25  15]\n",
      " [  8   2   3   1   1   0   4   2   6   6   0   3   1   8   8  10  40  23\n",
      "  113  20]\n",
      " [ 47   1   0   0   3   0   2   6   4   3   4   2   0   5   3  29  32  11\n",
      "   17  67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred_sklearn))\n",
    "print(confusion_matrix(y_test,y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionary = fit_func(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model = predict(x_test,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.64      0.76       233\n",
      "           2       0.82      0.74      0.78       253\n",
      "           3       0.67      0.41      0.51       249\n",
      "           4       0.61      0.45      0.52       240\n",
      "           5       0.60      0.46      0.52       236\n",
      "           6       0.76      0.88      0.82       240\n",
      "           7       0.74      0.81      0.77       261\n",
      "           8       0.83      0.88      0.85       269\n",
      "           9       0.84      0.93      0.89       284\n",
      "          10       0.68      0.59      0.63       248\n",
      "          11       0.73      0.71      0.72       231\n",
      "          12       0.82      0.85      0.84       233\n",
      "          13       0.70      0.85      0.77       244\n",
      "          14       0.73      0.86      0.79       256\n",
      "          15       0.64      0.82      0.72       246\n",
      "          16       0.69      0.69      0.69       252\n",
      "          17       0.49      0.62      0.55       249\n",
      "          18       0.66      0.57      0.61       281\n",
      "          19       0.52      0.45      0.49       259\n",
      "          20       0.37      0.48      0.42       236\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      5000\n",
      "   macro avg       0.69      0.68      0.68      5000\n",
      "weighted avg       0.69      0.69      0.68      5000\n",
      "\n",
      "[[150   1   0   0   0   1   0   0   1   0   1   0   2   2   2   7   8   3\n",
      "    0  55]\n",
      " [  0 187  11   2   6  10   3   1   0   2   1   1   3   5  13   0   1   1\n",
      "    0   6]\n",
      " [  0  12 101  21  13  31   9   3   5   2   1   6   8   6  17   1   3   3\n",
      "    2   5]\n",
      " [  0   7  18 107  40   3  16   8   7   0   1   3  19   4   1   2   1   0\n",
      "    0   3]\n",
      " [  0   1  11  29 109   3  19   7   2   2   1   2  22   8  10   0   9   1\n",
      "    0   0]\n",
      " [  0   4   2   2   4 210   2   1   2   0   0   1   6   1   3   0   1   0\n",
      "    1   0]\n",
      " [  0   0   1   9   5   0 211   7   1   2   3   1   4   1  10   0   2   2\n",
      "    1   1]\n",
      " [  0   0   1   0   0   1   4 236   3   0   0   0   7   0   1   1   6   3\n",
      "    4   2]\n",
      " [  0   0   0   0   0   0   1   3 265   0   0   0   0   3   3   0   2   1\n",
      "    5   1]\n",
      " [  0   0   0   0   0   5   5   3   4 146  44   1   2   7   8   2   1   4\n",
      "    4  12]\n",
      " [  0   0   1   0   0   1   2   0   4  36 165   2   0   1   3   1   5   4\n",
      "    2   4]\n",
      " [  0   2   0   0   1   4   0   1   0   1   0 198   2   2   2   2   7   1\n",
      "    4   6]\n",
      " [  0   4   1   2   3   1   6   5   3   0   0   0 208   5   1   3   1   0\n",
      "    0   1]\n",
      " [  0   2   1   0   1   1   1   0   3   2   6   0   1 219   6   1   2   2\n",
      "    1   7]\n",
      " [  0   4   0   0   0   4   1   1   2   0   1   2   6   4 202   0   4   2\n",
      "    6   7]\n",
      " [  1   0   1   0   0   0   1   1   1   6   0   4   3   8   5 175   6  12\n",
      "    9  19]\n",
      " [  0   1   0   0   1   0   0   3   4   3   1   7   1   6   6   5 154  13\n",
      "   26  18]\n",
      " [  3   2   0   0   0   0   4   1   3   6   1   8   2   5   8   8  18 160\n",
      "   26  26]\n",
      " [  2   1   1   2   0   0   2   2   2   6   0   3   0   7  13  13  45  18\n",
      "  117  25]\n",
      " [  8   1   0   0   0   0   0   2   2   2   1   2   0   4   4  31  36  14\n",
      "   15 114]]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred_model))\n",
    "print(classification_report(y_test,y_pred_model))\n",
    "print(confusion_matrix(y_test,y_pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
